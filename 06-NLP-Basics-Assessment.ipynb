{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ère partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Créer un objet Doc à partir du fichier `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "fichier_owlcreek = r\"C:\\Users\\leonm\\Downloads\\TP\\TP\\TP6\\owlcreek.txt\"\n",
    "\n",
    "with open(fichier_owlcreek, 'r', encoding='utf-8') as f:\n",
    "    texte = f.read()\n",
    "\n",
    "doc = nlp(texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Combien de tokens sont contenus dans le fichier ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombre_tokens = len(doc)\n",
    "nombre_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combien de phrases sont contenues dans le fichier ?<br>HINT: Vous devez d'abord créer une liste !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = list(doc.sents)\n",
    "nombre_phrases = len(phrases)\n",
    "nombre_phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imprimer la deuxième phrase du document**<br> HINT: L'indexation commence à zéro et le titre compte comme la première phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The man's hands were behind\\nhis back, the wrists bound with a cord.  \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(texte)\n",
    "# Obtenir  la liste des phrases dans le document\n",
    "phrases = list(doc.sents)\n",
    "# Imprimez la deuxième phrase du document\n",
    "deuxieme_phrase = phrases[1]\n",
    "deuxieme_phrase.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pour chaque élément de la phrase ci-dessus, imprimez son`text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Faire en sorte que les valeurs s'alignent en colonnes dans la sortie imprimée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The             DET        det             the\n",
      "man             NOUN       poss            man\n",
      "'s              PART       case            's\n",
      "hands           NOUN       nsubj           hand\n",
      "were            AUX        ROOT            be\n",
      "behind          ADP        prep            behind\n",
      "\n",
      "               SPACE      dep             \n",
      "\n",
      "his             PRON       poss            his\n",
      "back            NOUN       pobj            back\n",
      ",               PUNCT      punct           ,\n",
      "the             DET        det             the\n",
      "wrists          NOUN       appos           wrist\n",
      "bound           VERB       acl             bind\n",
      "with            ADP        prep            with\n",
      "a               DET        det             a\n",
      "cord            NOUN       pobj            cord\n",
      ".               PUNCT      punct           .\n",
      "                SPACE      dep              \n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION:\n",
    "for token in deuxieme_phrase:\n",
    "    print(f\"{token.text.ljust(15)} {token.pos_.ljust(10)} {token.dep_.ljust(15)} {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET det\n",
      "man NOUN poss\n",
      "'s PART case\n",
      "hands NOUN nsubj\n",
      "were AUX ROOT\n",
      "behind ADP prep\n",
      "\n",
      " SPACE dep\n",
      "his PRON poss\n",
      "back NOUN pobj\n",
      ", PUNCT punct\n",
      "the DET det\n",
      "wrists NOUN appos\n",
      "bound VERB acl\n",
      "with ADP prep\n",
      "a DET det\n",
      "cord NOUN pobj\n",
      ". PUNCT punct\n",
      "  SPACE dep\n"
     ]
    }
   ],
   "source": [
    "# CHALLENGE SOLUTION:\n",
    "for token in deuxieme_phrase:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ème partie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Tokenisation de phrases avec SpaCy\n",
    "\n",
    "Question : Créez une phrase complexe en anglais et utilisez SpaCy pour la tokeniser en phrases individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase 1: Tokenization is a fundamental step in NLP.\n",
      "Phrase 2: It involves breaking down a text into smaller units called tokens.\n"
     ]
    }
   ],
   "source": [
    "# Créez une phrase complexe\n",
    "phrase_complexe = \"Tokenization is a fundamental step in NLP. It involves breaking down a text into smaller units called tokens.\"\n",
    "\n",
    "# Utilisez SpaCy pour tokeniser la phrase en phrases individuelles\n",
    "doc = nlp(phrase_complexe)\n",
    "\n",
    "# Affichez chaque phrase individuelle\n",
    "for i, phrase in enumerate(doc.sents, 1):\n",
    "    print(f\"Phrase {i}: {phrase.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Utilisez SpaCy pour tokeniser la phrase complexe en mots.\n",
    "\n",
    "Question : Quels avantages offre SpaCy par rapport à d'autres bibliothèques pour la tokenisation?\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_tokenizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization\n",
      "is\n",
      "a\n",
      "fundamental\n",
      "step\n",
      "in\n",
      "NLP\n",
      ".\n",
      "It\n",
      "involves\n",
      "breaking\n",
      "down\n",
      "a\n",
      "text\n",
      "into\n",
      "smaller\n",
      "units\n",
      "called\n",
      "tokens\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation en mots avec SpaCy\n",
    "phrase_complexe = \"Tokenization is a fundamental step in NLP. It involves breaking down a text into smaller units called tokens.\"\n",
    "\n",
    "doc = nlp(phrase_complexe)\n",
    "\n",
    "# Affichez chaque mot\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spacy_tokenizer(phrase):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    tokens = [token.text.lower() for token in nlp(phrase)]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'nlp', '.', 'it', 'involves', 'breaking', 'down', 'a', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "# Fonction de tokenisation personnalisée avec SpaCy\n",
    "\n",
    "resultat_tokens = custom_spacy_tokenizer(phrase_complexe)\n",
    "\n",
    "print(resultat_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3\n",
    "\n",
    "Question : Utilisez SpaCy pour extraire les lemmes et les POS de la phrase complexe.\n",
    "\n",
    "Question : Comment SpaCy gère-t-il la tokenisation des entités nommées? Testez cela sur une phrase contenant une entité nommée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple           ORG\n",
      "is              \n",
      "a               \n",
      "major           \n",
      "technology      \n",
      "company         \n",
      ".               \n"
     ]
    }
   ],
   "source": [
    "# Tokenisation des entités nommées avec SpaCy\n",
    "text_with_entity = \"Apple is a major technology company.\"\n",
    "\n",
    "# Utilisez SpaCy pour traiter la phrase\n",
    "doc_with_entity = nlp(text_with_entity)\n",
    "\n",
    "\n",
    "for token in doc_with_entity:\n",
    "    print(f\"{token.text.ljust(15)} {token.ent_type_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer le stemming sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.', 'it', 'involve', 'reduce', 'word', 'to', 'their', 'base', 'form', '.']\n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "text = \"Stemming is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "# Utilisez SpaCy pour traiter la phrase\n",
    "doc = nlp(text)\n",
    "\n",
    "# Affichez les mots lemmatisés \n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_stemmer qui prend une phrase en entrée, utilise SpaCy pour effectuer le stemming, et renvoie les stems en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_stemmer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stemming personnalisée avec SpaCy\n",
    "\n",
    "\n",
    "def custom_spacy_stemmer(phrase):\n",
    "    doc = nlp(phrase)\n",
    "    stems = [token.lemma_.lower() for token in doc]\n",
    "    return stems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'be', 'an', 'essential', 'part', 'of', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "phrase_test = \"Stemming is an essential part of natural language processing.\"\n",
    "resultat_stems = custom_spacy_stemmer(phrase_test)\n",
    "print(resultat_stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 \n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer la lemmatisation sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization   lemmatization\n",
      "is              be\n",
      "an              an\n",
      "essential       essential\n",
      "part            part\n",
      "of              of\n",
      "natural         natural\n",
      "language        language\n",
      "processing      processing\n",
      ".               .\n",
      "It              it\n",
      "involves        involve\n",
      "reducing        reduce\n",
      "words           word\n",
      "to              to\n",
      "their           their\n",
      "base            base\n",
      "form            form\n",
      ".               .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# # Téléchargement du modèle de langue anglaise\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Affichez les mots lemmatisés (forme de base)\n",
    "for token in doc:\n",
    "    print(f\"{token.text.ljust(15)} {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_lemmatizer qui prend une phrase en entrée, utilise SpaCy pour effectuer la lemmatisation, et renvoie les lemmes en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_lemmatizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "def custom_spacy_lemmatizer(phrase):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(phrase)\n",
    "    lemmes = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    return lemmes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lemmatization',\n",
       " 'be',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'part',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "phrase_test = \"Lemmatization is an essential part of natural language processing.\"\n",
    "resultat_lemmes = custom_spacy_lemmatizer(phrase_test)\n",
    "\n",
    "\n",
    "resultat_lemmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour filtrer les stopwords de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Téléchargement du modèle de langue anglaise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Filtrer les stopwords\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Affichez les mots sans les stopwords\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction remove_stopwords qui prend une phrase en entrée, utilise SpaCy pour filtrer les stopwords, et renvoie les mots restants en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction remove_stopwords à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de filtrage des stopwords avec SpaCy\n",
    "def remove_stopwords(phrase):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(phrase)\n",
    "    \n",
    "    # Filtrer les stopwords et renvoyer les mots restants en minuscules\n",
    "    mots_sans_stopwords = [token.text.lower() for token in doc if not token.is_stop]\n",
    "    \n",
    "    return mots_sans_stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'involves',\n",
       " 'automatic',\n",
       " 'processing',\n",
       " 'human',\n",
       " 'language',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_test = \"Natural language processing involves the automatic processing of human language.\"\n",
    "resultat_sans_stopwords = remove_stopwords(phrase_test)\n",
    "\n",
    "# Affichez les résultats\n",
    "resultat_sans_stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez une fonction 'preprocess_text_spacy' qui applique les méthodes de preprocessing vues en cours et permet d'obtenir les résultats prétraités suivants :\n",
    "\n",
    "NB : n'oublier pas de traiter les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess_text_spacy(text):\n",
    "     # Chargez le modèle de langue SpaCy pour l'anglais\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Tokenisation, lemmatisation et suppression des stopwords\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n",
    "    \n",
    "    # Suppression des caractères spéciaux\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Joindre les tokens en une phrase\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing involve automatic processing human language'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_preprocess = \"Natural language processing involves the automatic processing of human language.\"\n",
    "resultat_pretraitement = preprocess_text_spacy(text_to_preprocess)\n",
    "\n",
    "# Affichez les résultats\n",
    "resultat_pretraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello know natural language processing fascinating field'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'utilisation avec SpaCy\n",
    "raw_text = \"Hello, I know that Natural Language Processing is a fascinating field!!!\"\n",
    "resultat_pretraitement = preprocess_text_spacy(raw_text)\n",
    "\n",
    "# Affichez les résultats\n",
    "resultat_pretraitement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
